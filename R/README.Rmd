---
title: "Freeriding the Metaverse"
output: 
  github_document
---

<!--
# To render the output to another directory, use the following lines in the YAML header
# Thanks https://stackoverflow.com/a/71826276
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_dir = "../"
    )
  })
-->

Fast fashion is associated with waste, micro-plastics and a waste of resources. [Digital fashion](https://www.netflix.com/watch/81197117) promises to be superior to conventional (fast) fashion in all of these regards as it allows consumers, for instance, to 
try on outfits virtually such that they can better assess the clothing even before its physical twins have been shipped. Another scenario is that it reduces the consumption of physical fashion since the virtual products suffice as a means to represent oneself in social media or the _metaverse_.

Along these lines several different stakeholders (including major brands such as [Forever21](https://twitter.com/Forever21/status/1507729045823234056) and [Tommy Hilfiger](https://twitter.com/TommyHilfiger/status/1507025541945245708)) started to experiment at the intersection of non fungible tokens (NFTs) and fashion organizing the first ever _Metaverse Fashion Week_ (MVFW) online in March 2022. In addition, the 2022's New York Fashion Week is also accompanied by the [release of NFTs that match physical products](https://www.forbes.com/sites/yolarobert1/2022/09/10/alo-yoga-debuts-its-first-ready-to-wear-collection-with-a-limited-edition-nft-at-new-york-fashion-week/).

Due to this recency, we scrape twitter data to better understand the key stakeholders of the MVFW.

# Contents

The second section shows **[how we processed the data](#data)** while the third section contains a brief **[exploratory analysis](#exploration)**. I also show [random samples of tweets by domain](#examplary-tweets).

---

_(The following four code chunks set up the document.)_

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r settings}
options(stringsAsFactors = FALSE)
invisible(Sys.setlocale(category = "LC_ALL", locale = "C"))
set.seed(42)
```

```{r packages}
# install.packages("pacman")
pacman::p_load(magrittr, data.table, stringr, lubridate, # overviewR,
               ggplot2, MetBrewer, knitr, fs, purrr,
               qdapRegex)
```

```{r design}
# ggplot layout
layout <- theme(panel.background = element_rect(fill = "transparent", color = NA),
                plot.background = element_rect(fill = "transparent", color = NA),
                panel.grid = element_blank(),
                panel.grid.major.y = element_blank(),
                legend.key = element_rect(fill = "transparent"),
                axis.line = element_line(size = 0.25),
                axis.ticks = element_line(size = 0.25),
                plot.caption = element_text(colour = "#555555"),
                legend.title = element_blank()
)

# color
# colors <- met.brewer(name="Tam",n=7,type="discrete")
cPrimary = "#00802F"
cSecondary = "#EB6969"
cInfo = "#FFF04B"
cDanger <- "#EB6969"
```

```{r constants}
STARTDATE <- as.Date("2022-03-23")
ENDDATE   <- as.Date("2022-03-27")
```

# Data

We query data using [TWINT](https://github.com/twintproject/twint) and the command line interface. Because twitter limits scrolls while browsing a timeline, one can scrape about [3200 tweets per query](https://github.com/twintproject/twint#limits-imposed-by-twitter). For this reason, we run multiple queries and vary them in two dimensions: the [time frames](#time-frames) as well as the [user names](#brands).

## Time frames

The following queries scrape data from all users that either mentioned `mvfw` or `metaverse fashion week` in different periods between 2022-02-23 and 2022-04-27. The results are stored in different `.csv` files, loaded and row-binded to one data.table called `tmp`.

```
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-02-23" --until "2022-03-02" --lang "en" -o dev/mvfw/data/timeFrames/tmp1.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-02" --until "2022-03-09" --lang "en" -o dev/mvfw/data/timeFrames/tmp2.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-09" --until "2022-03-16" --lang "en" -o dev/mvfw/data/timeFrames/tmp3.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-16" --until "2022-03-23" --lang "en" -o dev/mvfw/data/timeFrames/tmp4.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-23" --until "2022-03-28" --lang "en" -o dev/mvfw/data/timeFrames/tmp5.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-28" --until "2022-04-04" --lang "en" -o dev/mvfw/data/timeFrames/tmp6.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-04-04" --until "2022-04-11" --lang "en" -o dev/mvfw/data/timeFrames/tmp7.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-04-11" --until "2022-04-18" --lang "en" -o dev/mvfw/data/timeFrames/tmp8.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-04-18" --until "2022-04-27" --lang "en" -o dev/mvfw/data/timeFrames/tmp9.csv --csv
```

```{r readBatchesTiming}
data_path    <- "../data/timeFrames/"
file_paths   <- fs::dir_ls(path = data_path, glob = "*.csv")
object_names <- str_replace_all(string = file_paths,
                                pattern = paste0(data_path, "|\\.csv"),
                                replacement = "")
datasets     <- purrr::map(file_paths, read.csv, sep = "\t")
tmp <- data.table::rbindlist(l = datasets)
```

## Brands

In addition, we scrape data by usernames that represent brands we identified synthesizing different news articles that covered the event.^[e.g. [1](https://www.voguebusiness.com/technology/metaverse-fashion-week-the-hits-and-misses), [2](https://www.vogue.com/article/metaverse-fashion-week-decentraland), [3](https://decentraland.org/blog/announcements/metaverse-fashion-week-is-here/#flagship-stores), [4](https://www.lifestyleasia.com/ind/gear/tech/highlights-from-2022-metaverse-fashion-week/) and [5](https://metaversefashionweek.com/)]

This yields a list of 27 usernames.^[tommyhilfiger, forever21, esteelauder, KarlLagerfeld, McQueen, dkny, dressxcom, dolcegabbana, EtroOfficial, FranckMuller, Selfridges, Bulova, PacoRabanne, ArtistVasarely, PUMA, PerryEllis, Fred_Segal, VAULTswiss, MissJAlexander, marc0matic, Charles_Keith, priveporter, philipp_plein, ElieSaabWorld, HoganBrand, IOCNFTs, dundaslondon]
Replace `[username]` by the desired brand to scrape the corresponding tweets between 2022-02-23 and 2022-04-24. The resulting `.csv` files are then again loaded and row-binded to one data.table called `brands`.

```
twint -u [username] -s "mvfw OR (Metaverse Fashion Week)" --since "2022-02-23" --until "2022-04-24" --lang "en" -o dev/mvfw/data/brands/[username].csv --csv
```
<!--
=CONCAT("twint -u ", B2, " -s "'mvfw OR (Metaverse Fashion Week)" --since "'2022-02-23" --until "'2022-04-24" --lang "'en" -o dev/mvfw/data/brands/", B2, ".csv --csv")
-->

```{r readBatchesBrands}
data_path   <- "../data/brands/"
file_paths  <- fs::dir_ls(path = data_path, glob = "*.csv")
datasets    <- purrr::map(file_paths, read.csv, sep = "\t")
brands <- data.table::rbindlist(l = datasets)
```

## Refactor

First, row-bind both data.tables, i.e. `brands` and `tmp`.

```{r combineData}
temp <- data.table::rbindlist(l = list(tmp, brands))
```

Then make sure to use a common character encoding, remove ULRs before you remove duplicate tweets. Subsequently, add an ID, work on time and date formats, etc.

```{r transformData, warning = FALSE}
# String clean up 
temp[, tweet := iconv(tweet, "latin1", "ASCII", sub = "")]
temp[, tweet := rm_url(tweet,                    # remove URLs
                      pattern = pastex("@rm_twitter_url", "@rm_url"))]

# subset english sample of UNIQUE tweets
en <- temp[language == "en"] %>% unique(by = "tweet")

# create distinc ID
en[, doc_id := .I]

# change date & time format
en[, created_at := str_sub(string = created_at,
                           start  = 1,
                           end    = 19) %>% ymd_hms()]
en[, date := ymd(date)]

# store mentions (@....)
en[, customMentions := str_extract_all(string = tweet,
                                       pattern = "@\\S+")]
en[customMentions == "character(0)", 
   customMentions := NA]
en[, nMentions := str_count(string = customMentions, pattern = "@")]
```

```{r addTimings}
en[, timing := "after"]
en[date <= ENDDATE, timing := "during"]
en[date < STARTDATE, timing := "before"]

en[, timing := factor(timing,
                      ordered = TRUE,
                      levels  = c("before", "during", "after"))]
```

```{r}
# re-arrange data for corpus
data <- en[date <= as.Date("2022-04-24"), # due to brands query
           .(doc_id,
             text = tweet,
             hashtags,
             cashtags,
             username,
             mentions,
             customMentions,
             nMentions,
             name,
             place,
             urls,
             photos,
             video,
             geo,
             timing,
             date,
             created_at,
             timezone,
             replies_count,
             retweets_count,
             likes_count,
             language,
             id,
             conversation_id,
             retweet_id)]
```

## Classification
Now, we'll tag users such that they fall into different categories of stakeholders. More precisely, I differentiate between two different domains: `web3` and `fashion`. In addition, I also create a second dimension -- the `type` describing either `platform providers`, `content creators`, who one could also describe as evangelists or influencers, as well as `fashion creators`.

Note that:^[Note to myself: create table, if two dimensions are really required.]

- conventional brands are described by `domain =="fashion" & type=="fashion creators"`. 
- fashion influencers are described by `domain =="fashion" & type=="content creators"`.
- web3 evangelists are described by `domain =="web3" & type=="content creators"`.
- digital fashion brands are described by `domain =="web3" & type=="fashion creators"`.

I've done the classification in a semi-automated way by focusing on the `username`s.

### Automated heuristics

First, I apply regular expressions to tag web3 content creators.

```{r regexWeb3}
data[str_detect(string = username,
                pattern = "nfts?|crypt|krypt|meta|block|coin"),
     `:=`(domain = "web3",
          type   = "content creators")]
```

Next, I use a similar approach to tag fashion and beauty related content creators.

```{r regexFashion}
data[str_detect(string = username,
                pattern = "fashion|beauty|luxury"),
     `:=`(domain = "fashion",
          type   = "content creators")]
```

This yields many usernames that fall into neither of these categories. This is where the manual part starts.

### Manual inspection

The following figure shows a density plot that illustrates the number of likes a user received.^[I limit this figure to users that received at least one but less than 1000 likes.]

```{r warning = FALSE}
tmp <- data[is.na(domain), 
     .(likes = sum(likes_count, na.rm = TRUE)),
     by = username][order(-likes)]

ggplot(data = tmp[likes > 0 & likes < 1000],
       mapping = aes(x = likes)) +
  # geom_histogram(fill = cPrimary, binwidth = 5) +
  geom_density(fill = cPrimary, alpha = 0.66, col = cPrimary) +
  scale_y_continuous(limits = c(0, NA),
                     expand = c(0, NA)) +
  layout

```

```{r}
known   <- tmp[1:100, sum(likes)]
overall <- tmp[, sum(likes)]
share   <- round(100*known/overall)
```

Next, I display a table of the 100 most relevant users, measured by the number of likes they received for all their posts in our data. These 100 users account for `r share`% of the likes we observe in our data that are not yet categorized by the automatic approach outlined [above](#automated-heuristics).

```{r showNA}
tmp %>% 
  head(100) %>%
  kable()
```


Subsequently, I search the top 100 users in twitter using a web browser, read their description (or biography) as well their posts in our data. Having reviewed all of them, I'll classify them by hand.

```{r tags}

# tag web3 freeriders
data[username %in% c("Deadfellaz", "gossapegirl", "asian_mint", "canessadcl",     # NAME WEB3 HERE!
                     "ericpi888", "itskac", "antisecretsoci2", "cmnnewsofficial",
                     "cathyhackl", # maybe too much fame to be a free rider?
                     "btctn", "_mannyalves", "maryanadcl", "martinshibuya",
                     "eagle_stephen_", "8sianmom", "tokens_com", "mrbathinape",
                     "michi_todd", "bitpanda", "brytehall", "universelle_io",
                     "kcain1982", "borgetsebastien", "barbarakahn", "diviproject",
                     "ziziverse", "astronotseth", "yannakis_dcl", "xpozd_io",
                     "tangpoko", "thesevens_7", "portionapp", "0xjoules",
                     "teenybod", "celinatech", "enilev", "siddharthakur",
                     "pedroguez__", "additionalrules", "media_diamante",
                     "altavagroup", "ww_ventures", "davidcash888", "move78studio",
                     "qdibs_eth", "realsophiarobot", "manadaiquiridcl",
                     "madamape", "decentralgames", "projectmediahq", "dcljasonx",
                     "mutani_io", "0xquiksilver", "reginaturbina", "jtv____",
                     "dogmandcl", "soultrydubs", "lingxing_dcl", "knownorigin_io",
                     "danitpeleg3d", "survive_p2e", "mgh_dao", "serenaelis_",
                     "koryptostylist",
                     "deadfellaz" # not sure where to put these guys since they have some sort of cooperation
                     ), 
     `:=`(domain = "web3",
          type   = "content creators")]

# tag web3 x fashion
data[username %in% c("thefabricant", "xrcouture", "auroboros_ltd", "wirelyss",    # NAME WEB3 FASHION HERE!
                     "polygondressing", "the_vogu", "shopcider", "houseofdaw",
                     "neuno_io", "stylexchange_io", "parzival_kazuto", "bitski"), 
     `:=`(domain = "web3",
          type   = "fashion creators")]

# tag content creators & media
data[username %in% c("thalia", "maghanmcd", "diamondhandbag", "realfaithtribe"   # NAME CONTENT CREATORS HERE!
                     ), 
     `:=`(domain = "fashion",
          type   = "content creators")]

# tag fashion brands
data[username %in% brands[, unique(username)], # NAME BRANDS HERE!
     `:=`(domain = "fashion",
          type   = "fashion creators")]

# tag platform- or ecosystem related users
data[username %in% c("decentraland", "bosonprotocol", "exclusible", "threedium",  # NAME PLATFORMS HERE!
                     "pangeadao", "dragoncityio", "whiterabbitgate"), 
     `:=`(domain = "web3",
          type   = "platform providers")]
```

The data contains `r format(tmp[, .N], big.mark = ".", decimal.mark = ",")` rows, each representing a tweet.  Its columns represent some IDs, meta information about URLs, retweets, etc. as well as the tweets itself (from which I removed URLs using `qdapRegex::rm_url()`). 

```{r saveData}
data <- data[,
             .(doc_id,
               text,
               username,
               domain,
               type,
               timing,
               date,
               created_at,
               replies_count,
               retweets_count,
               likes_count,
               mentions,
               customMentions,
               nMentions,
               name,
               place,
               urls,
               photos,
               video,
               geo,
               timezone,
               language,
               id,
               conversation_id,
               retweet_id,
               hashtags,
               cashtags)]

save(data, file = "../data/processed/mvfw.RData")
```


# Exploration

Here is a list of the 25 users who received the most likes. Almost all of them are centered around the web3 domain.

```{r}
data[, .(likes = sum(likes_count, na.rm = TRUE)), by = c("username", "domain", "type")][order(-likes)] %>% head(25) %>% kable()
```

## Tweets by domain

### Number of Tweets

```{r}
data[, .N, by = domain] %>% kable()
tmp <- data[, .N, by = c("username", "domain")]
ggplot(data = tmp,
       mapping = aes(x = domain, y = N, fill = domain)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(0, NA),
                     expand = c(0, NA)) +
  layout +
  labs(x = "")
```

### Number of Likes

```{r}
ggplot(data = data[, .(domain, likes_count)],
       mapping = aes(x = domain, y = likes_count, fill = domain)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(0, NA),
                     expand = c(0, NA)) +
  layout +
  labs(x = "")
```

### Number of Retweets

```{r}
ggplot(data = data[, .(domain, retweets_count)],
       mapping = aes(x = domain, y = retweets_count, fill = domain)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(0, NA),
                     expand = c(0, NA)) +
  layout +
  labs(x = "")
```

## Tweets by type

### Number of Tweets

```{r}
data[, .N, by = type] %>% kable()
tmp <- data[, .N, by = c("username", "type")]
ggplot(data = tmp,
       mapping = aes(x = type, y = N, fill = type)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(0, NA),
                     expand = c(0, NA)) +
  layout +
  labs(x = "")
```

### Number of Likes

```{r}
ggplot(data = data[, .(type, likes_count)],
       mapping = aes(x = type, y = likes_count, fill = type)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(0, NA),
                     expand = c(0, NA)) +
  layout +
  labs(x = "")
```

### Number of Retweets

```{r}
ggplot(data = data[, .(type, retweets_count)],
       mapping = aes(x = type, y = retweets_count, fill = type)) +
  geom_bar(stat = "identity") +
  scale_y_continuous(limits = c(0, NA),
                     expand = c(0, NA)) +
  layout +
  labs(x = "")
```


## Tweets by type x domain

```{r}
data[, .(`Number of tweets` = .N), by = c("domain", "type")] %>% kable()
data[, .(`Number of likes` = sum(likes_count)), by = c("domain", "type")] %>% kable()
data[, .(`Number of retweets` = sum(retweets_count)), by = c("domain", "type")] %>% kable()
```

# Timing

## Timing x domain

How much was posted before, during and after the MVFW?

```{r}
tmp <- data[!is.na(domain), .(N = .N), by = c("timing", "domain")][order(domain)]
tmp[, sum := sum(N), by = domain]
tmp[, share := paste0(round(100*N/sum), "%")]
tmp %>% kable()
```

Even though we queried the data such that the duration of the time `before` is exactly as long as the duration of the time `after` (+/-`r STARTDATE - data[, min(date)]` days), we count less tweets in both domains (web3 as well as fashion) posted after the MVFW took place.

```{r eval = FALSE}
data[, min(date)] - STARTDATE
data[, max(date)] - ENDDATE
```

The same holds true for the number of likes received...

```{r}
tmp <- data[!is.na(domain), .(N = sum(likes_count)), by = c("timing", "domain")][order(domain)]
tmp[, sum := sum(N), by = domain]
tmp[, share := paste0(round(100*N/sum), "%")]
tmp %>% kable()
```

...as well as for the number of retweets.

```{r}
tmp <- data[!is.na(domain), .(N = sum(retweets_count)), by = c("timing", "domain")][order(domain)]
tmp[, sum := sum(N), by = domain]
tmp[, share := paste0(round(100*N/sum), "%")]
tmp %>% kable()
```

However, both of these engagement metrics (`likes_count` and `retweets_count`) show different patterns across domains. The web3 domain triggered by far the most engagement before the event, while the fashion domain received the most attention during the event (on the basis of a comparable amount of tweets).

## Timing x type

How much was posted before, during and after the MVFW?

```{r}
tmp <- data[!is.na(domain), .(N = .N), by = c("timing", "type")][order(type)]
tmp[, sum := sum(N), by = type]
tmp[, share := paste0(round(100*N/sum), "%")]
tmp %>% kable()
```

# Examplary Tweets

## Fashion

```{r}
data[domain == "fashion" & type == "fashion creators", .(text, username)][sample(.N, 15)] %>% kable()
data[domain == "fashion" & type == "content creators", .(text, username)][sample(.N, 15)] %>% kable()
```

## Web3

```{r}
data[domain == "web3" & type == "fashion creators", .(text, username)][sample(.N, 15)] %>% kable()
data[domain == "web3" & type == "content creators", .(text, username)][sample(.N, 15)] %>% kable()
```
