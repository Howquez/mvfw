---
title: "Freeriding the Metaverse: Data Collection"
output: github_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r settings}
options(stringsAsFactors = FALSE)
invisible(Sys.setlocale(category = "LC_ALL", locale = "C"))
set.seed(42)
```

```{r packages}
# install.packages("pacman")
pacman::p_load(magrittr, data.table, stringr, lubridate, # overviewR,
               ggplot2, MetBrewer, knitr, fs, purrr,
               qdapRegex)
```

```{r constants}
STARTDATE <- as.Date("2022-03-23")
ENDDATE   <- as.Date("2022-03-27")
```

Fast fashion is associated with waste, micro-plastics and a waste of resources. [Digital fashion](https://www.netflix.com/watch/81197117) promises to be superior to conventional (fast) fashion in all of these regards as it allows consumers, for instance, to 
try on outfits virtually such that they can better assess the clothing even before its physical twins have been shipped. Another scenario is that it reduces the consumption of physical fashion since the virtual products suffice as a means to represent oneself in social media or the _metaverse_.

Along these lines several different stakeholders started to experiment at the intersection of non fungible tokens (NFTs) and fashion organizing the first ever _Metaverse Fashion Week_ (MVFW) online in March 2022. In addition, the 2022's New York Fashion Week is also accompanied by the [release of NFTs that match physical products](https://www.forbes.com/sites/yolarobert1/2022/09/10/alo-yoga-debuts-its-first-ready-to-wear-collection-with-a-limited-edition-nft-at-new-york-fashion-week/).

Due to this recency, we scrape twitter data to better understand the key stakeholders of the MVFW


# Data

We query data using [TWINT.](https://github.com/twintproject/twint) We use brands such as Tommy Hilfiger and Forever 21, who opened flagship stores in decentraland and of whom we know that they posted something about the MVFW, as a starting point to fine tune our query.

Using the following two queries, I eventually create two objects:

- **hilfiger** `twint -u TommyHilfiger -s "mvfw" --since "2022-02-23" --until "2022-04-24" --lang "en" -o dev/mvfw/data/Hilfigerdata.csv --csv`

- **forever21** `twint -u Forever21 -s "Metaverse Fashion Week" --since "2022-02-23" --until "2022-04-24" --lang "en" -o dev/mvfw/data/Forever21data.csv --csv`

```{r, eval = FALSE}
hilfiger <- read.csv("../data/Hilfigerdata.csv", sep = "\t") %>%
  data.table()
forever21 <- read.csv("../data/Forever21data.csv", sep = "\t") %>% 
  data.table()
```

These two objects illustrate that one should be able to retrieve relevant data using the search terms `msfw` as well as `metaverse fashion week`.^[Attempts to combine both search terms using `OR` failed.] I therefore, use these queries but 

1. omit the username specifications and 
2. reduce the time frame to the duration of the MVFW (because it seems as if larger querries yield sampled data).


- **tmp1** `twint -s "mvfw" --since "2022-03-24" --until "2022-03-27" --lang "en" -o dev/mvfw/data/MVFWdata1.csv --csv`

- **tmp2** `twint -s "Metaverse Fashion Week" --since "2022-03-24" --until "2022-03-27" --lang "en" -o dev/mvfw/data/MVFWdata2.csv --csv`


```{r readData}
tmp1 <- read.csv("../data/MVFWdata1.csv", sep = "\t") %>% 
  data.table()
tmp2 <- read.csv("../data/MVFWdata2.csv", sep = "\t") %>% 
  data.table()
tmp <- data.table::rbindlist(l = list(tmp1, tmp2))
```

Moving forward, we could apply this strategy to multiple (short) time periods and `rbind()` the resulting data objects. 

## Time Frames

```
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-02-23" --until "2022-03-02" --lang "en" -o dev/mvfw/data/timeFrames/tmp1.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-02" --until "2022-03-09" --lang "en" -o dev/mvfw/data/timeFrames/tmp2.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-09" --until "2022-03-16" --lang "en" -o dev/mvfw/data/timeFrames/tmp3.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-16" --until "2022-03-23" --lang "en" -o dev/mvfw/data/timeFrames/tmp4.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-23" --until "2022-03-28" --lang "en" -o dev/mvfw/data/timeFrames/tmp5.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-03-28" --until "2022-04-04" --lang "en" -o dev/mvfw/data/timeFrames/tmp6.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-04-04" --until "2022-04-11" --lang "en" -o dev/mvfw/data/timeFrames/tmp7.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-04-11" --until "2022-04-18" --lang "en" -o dev/mvfw/data/timeFrames/tmp8.csv --csv
twint -s "mvfw OR (Metaverse Fashion Week)" --since "2022-04-18" --until "2022-04-27" --lang "en" -o dev/mvfw/data/timeFrames/tmp9.csv --csv
```

```{r readBatchesTiming}
data_path    <- "../data/timeFrames/"
file_paths   <- fs::dir_ls(path = data_path, glob = "*.csv")
object_names <- str_replace_all(string = file_paths,
                                pattern = paste0(data_path, "|\\.csv"),
                                replacement = "")
datasets     <- purrr::map(file_paths, read.csv, sep = "\t")
tmp <- data.table::rbindlist(l = datasets)
```

## Brands

Additionally, one can scrape data by brands. To do so, I'll use the following query and replace `[username]`.

```
twint -u [username] -s "mvfw OR (Metaverse Fashion Week)" --since "2022-02-23" --until "2022-04-24" --lang "en" -o dev/mvfw/data/brands/[username].csv --csv
```
<!--
=CONCAT("twint -u ", B2, " -s "'mvfw OR (Metaverse Fashion Week)" --since "'2022-02-23" --until "'2022-04-24" --lang "'en" -o dev/mvfw/data/brands/", B2, ".csv --csv")
-->

```{r readBatchesBrands}
data_path   <- "../data/brands/"
file_paths  <- fs::dir_ls(path = data_path, glob = "*.csv")
datasets    <- purrr::map(file_paths, read.csv, sep = "\t")
brands <- data.table::rbindlist(l = datasets)
```

## Refactor

```{r combineData}
temp <- data.table::rbindlist(l = list(tmp, brands))
```

```{r transformData}
# String clean up 
temp[, tweet := iconv(tweet, "latin1", "ASCII", sub = "")]
temp[, tweet := rm_url(tweet,                    # remove URLs
                      pattern = pastex("@rm_twitter_url", "@rm_url"))]

# subset english sample of UNIQUE tweets
en <- tmp[language == "en"] %>% unique(by = "tweet")

# create distinc ID
en[, doc_id := .I]

# change date & time format
en[, created_at := str_sub(string = created_at,
                           start  = 1,
                           end    = 19) %>% ymd_hms()]
en[, date := ymd(date)]

# store mentions (@....)
en[, customMentions := str_extract_all(string = tweet,
                                       pattern = "@\\S+")]
en[customMentions == "character(0)", 
   customMentions := NA]
en[, nMentions := str_count(string = customMentions, pattern = "@")]
```

```{r}
# tag web3 usernames by regex
en[str_detect(string = username,
                pattern = "nfts?|crypt|krypt|meta|block|coin"),
     domain := "web3"]

# tag fashion-related usernames by regex
en[str_detect(string = username,
                pattern = "fashion|beauty"),
     domain := "fashion"]

# tag web3 freeriders
en[username %in% c("Deadfellaz", "gossapegirl", "asian_mint", "canessadcl",     # NAME WEB3 HERE!
                   "ericpi888", "itskac", "antisecretsoci2", "cmnnewsofficial",
                   "cathyhackl", # maybe too much fame to be a free rider?
                   "btctn", "_mannyalves", "maryanadcl", "martinshibuya",
                   "eagle_stephen_", "8sianmom", "tokens_com", "mrbathinape",
                   "michi_todd", "bitpanda", "brytehall", "universelle_io",
                   "kcain1982", "borgetsebastien", "barbarakahn", "diviproject",
                   "ziziverse", "astronotseth", "yannakis_dcl", "xpozd_io",
                   "tangpoko", "thesevens_7", "portionapp", "0xjoules",
                   "teenybod", "celinatech", "enilev", "siddharthakur",
                   "pedroguez__"), 
     domain := "web3"]

# tag web3 x fashion
en[username %in% c("thefabricant", "xrcouture", "auroboros_ltd", "wirelyss",    # NAME WEB3 FASHION HERE!
                   "polygondressing", "the_vogu", "shopcider", "houseofdaw",
                   "neuno_io", "stylexchange_io"), 
     domain := "digital fashion"]

# tag content creators & media
en[username %in% c("thalia", "maghanmcd", "diamondhandbag", "realfaithtribe"   # NAME CONTENT CREATORS HERE!
                   ), 
     domain := "content creators"]

# tag fashion brands
en[username %in% brands[, unique(username)], # NAME BRANDS HERE!
     domain := "brands"]

# tag platform- or ecosystem related users
en[username %in% c("decentraland", "bosonprotocol", "exclusible", "threedium",  # NAME PLATFORMS HERE!
                   "pangeadao", "dragoncityio", "whiterabbitgate"), 
     domain := "platform"]

# manual inspection
en[username %in% c("additionalrules", "media_diamante"),
     domain := "web3"]
```

```{r}
# re-arrange data for corpus
data <- en[,
           .(doc_id,
             text = tweet,
             hashtags,
             cashtags,
             domain,
             username,
             mentions,
             customMentions,
             nMentions,
             name,
             place,
             urls,
             photos,
             video,
             geo,
             created_at,
             timezone,
             replies_count,
             retweets_count,
             likes_count,
             language,
             id,
             conversation_id,
             retweet_id)]

```

The data contains `r format(tmp[, .N], big.mark = ".", decimal.mark = ",")` rows, each representing a tweet.  Its columns represent some IDs, meta information about URLs, retweets, etc. as well as the tweets itself (from which I removed URLs using `qdapRegex::rm_url()`). 

# Inspection

Here is a list of the 50 users that received the most likes.

```{r}
# data[, .(likes = sum(likes_count, na.rm = TRUE)), by = c("username", "domain")][order(-likes)] %>% head(50) %>% kable()
data[is.na(domain), 
     .(likes = sum(likes_count, na.rm = TRUE)),
     by = username][order(-likes)] %>% 
  head(100) %>%
  kable()
```

```{r}

```

# To do

- Identify Brands

- Identify content creators



